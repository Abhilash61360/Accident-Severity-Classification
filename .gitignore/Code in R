rm(list=ls(all=TRUE))

#Set working directory
setwd("C:\\Users\\Abhilash\\Downloads\\20170916_Batch_28_CSE9099c_PHD_MLProblem\\20170916_Batch 28_CSE9099c_PHD_MLProblem")

#Read file
Accident_data_train<- read.csv("train_PHD.csv",header=TRUE)
Accident_data_validation<- read.csv("validation_PHD.csv",header=TRUE)
Accident_data_test<- read.csv("test_NoTarget_PHD.csv",header=TRUE)

#Preprocessing steps are the same thats submitted yesterday

#Mlr library gives a very better view of the data put together
library(mlr)
summarizeColumns(Accident_data_train)

#Is collision ref no unique in entire dataset?
#Total no of records
nrow(Accident_data_train)
#8562
length(unique(Accident_data_train$Collision.Reference.No.))
#8562


#Exclude columns , list down the numbers of columns to be excluded
#Collision refernce number can be excluded as it is just an identity column
excl_col=c(1)
datainit=Accident_data_train[-excl_col]
datavalid=Accident_data_validation[-excl_col]
datatest=Accident_data_test[-excl_col]


#Columns with more than 30% rows Nulls
col_nul=apply(datainit,2,FUN=function(x){(sum(is.na(x)))})
col_nul_var=which(col_nul>0.3*nrow(datainit))

#Rows with more than 30% columns nulls
row_nul=apply(datainit,1,FUN=function(x){(sum(is.na(x)))})
row_nul_var=which(row_nul>0.3*ncol(datainit))

#Columns having same values across all rows
rem_col=apply(datainit,2,FUN=function(x){(length(unique(x)))})
rem_col_var=which(rem_col==1)

#Remove if found any
if(length(col_nul_var)>0) {  datainit=datainit[-col_nul_var] }
if(length(row_nul_var)>0) { datainit=datainit[-row_nul_var,] }
if(length(rem_col_var)>0) { datainit=datainit[-rem_col_var] }
#No records deleted with above criteria




##Check nulls, Remove records which has Target Nulls
sum(is.na(datainit))
#1997
colSums(is.na(datainit))
#No Nulls in target columns
#Hours of collision,pedestrian,speccial conditions etc are having some 250 approx values unknown, should be ok to compute using KNN

#Validation dataset
sum(is.na(datavalid))
#420
colSums(is.na(datavalid))

#Now that we removed collision refernce num, see if we have any duplicate data
#Remove duplicates
nrow(unique(datainit))
nrow(datainit)
#One duplicate record found


#Remove duplicate
datainit=unique(datainit)


##Impute the missing data (only when you find missing data)
#Go with central Imputation so that you don't have to disturb the data structures
dataorig=datainit #Backup point 1
library(DMwR)
datainit<-centralImputation(datainit)
datavalid<-centralImputation(datavalid)
#datatest<-centralImputation(datatest)


#Target variable and closer look
table(datainit$Collision.Severity)
prop.table(table(datainit$Collision.Severity))


table(datainit$Collision.Severity,datainit$Policing.Area)
#Nothing so unusual with one particular policing for severity 1,2
#severity 3 are observed at relatively higher numbers at 'BELC'
#we an probably try exploring this for better accuracy


table(datainit$Collision.Severity,datainit$Weekday.of.Collision)
#weekends low numbers are registered, understandable 

table(datainit$Collision.Severity,datainit$Month.of.Collision)
#slight high during November and December


table(datainit$Collision.Severity,datainit$Month.of.Collision)
#slight high during November and December

table(datainit$Collision.Severity,datainit$Hour.of.Collision.24.hour)
#high numbers between peak hours, 8 am to 6 pm

table(datainit$Collision.Severity,datainit$Carriageway.Type)
#Carriage type 13, need to look into this, has highest number of accidents, significant factor

table(datainit$Collision.Severity,datainit$Speed.Limit)
#Majority of accidents between 30-60 kmph speed

table(datainit$Collision.Severity,datainit$Junction.Detail)
#Junction 1 and 12 , lot of accidents, one more significant factor

table(datainit$Collision.Severity,datainit$Junction.Control)
#1 and 7 are contributing for more accidents

table(datainit$Collision.Severity,datainit$Junction.Control,datainit$Junction.Detail)
#some interesting insights here

table(datainit$Collision.Severity,datainit$Pedestrian.Crossing.Human.Control)
table(datainit$Collision.Severity,datainit$Pedestrian.Crossing.Physical.Control)
#Majority for 1

table(datainit$Collision.Severity,datainit$Light.Conditions)
#Majority for 1

table(datainit$Collision.Severity,datainit$Weather.Conditions)
#weather ondistions 1,2,8,9,10 shows high numbers


table(datainit$Collision.Severity,datainit$Road.Surface.Conditions)
#Road surface 1,2 shows 80% of the registered numbers, important factor

table(datainit$Collision.Severity,datainit$Special.Conditions.at.Site)
#special condition 1 shows majority of accidents


#
levelvec=vector()
for (i in names(datainit))
{
  levelvec[i]=unique(datainit[i])
}

levelvec
#shows each column and levels in it


#Check for NearZero Variance Columns and remove them if needed
nzv <- caret::nearZeroVar(datainit, saveMetrics= TRUE)
nzv
nzv <- caret::nearZeroVar(datainit)
nzv
#datainit <- datainit[, -nzv]
#datavalid <- datavalid[, -nzv]

#columns 11, 16 Pedestrian.Crossing.Human.Control,Special.Conditions.at.Site can be removed
#verified the columns against severity and this column can be removed
#for the data volume we have, we can also give a try retaining these columns too while building the model


Features <- names(datainit)
datatest <- datatest[,(names(datatest) %in% Features)]

databackup1=datainit #checkpoint1
#datainit=databackup1


## High correlation columns identification
cordata=datainit
#Converting Factor to numeric on training dataset
fac_var=sapply(cordata,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){cordata[i]=as.numeric(unlist(cordata[i]))}

descrCor <- cor(cordata)
summary(descrCor[upper.tri(descrCor)])
highlyCorDescr <- caret::findCorrelation(descrCor, cutoff = .70)
#if(length(highlyCorDescr)>0) { datainit=datainit[,-highlyCorDescr] }
#if(length(highlyCorDescr)>0) { datavalid=datavalid[,-highlyCorDescr] }

#Junction.Detail column can be removed to as it is highly corelated, we can try retaining it too
#Features <- names(datainit)
#datatest <- datatest[,(names(datatest) %in% Features)]

##Linear Combos (Only for Numeric)
comboInfo <- caret::findLinearCombos(cordata)
comboInfo
#No columns observed


#outliers visualization
train=datainit
valid=datavalid
test=datatest
#Converting Factor to numeric on training dataset
fac_var=sapply(train,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){train[i]=as.numeric(unlist(train[i]))}
str(train)

#Converting Factor to numeric on validation dataset
fac_var=sapply(valid,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){valid[i]=as.numeric(unlist(valid[i]))}
str(valid)

#Converting Factor to numeric on validation dataset
fac_var=sapply(test,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){test[i]=as.numeric(unlist(test[i]))}
str(test)

#Train Data distribution
df2=train
names(df2)=c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16")
df2=scale(df2)
df3=data.frame(col=boxplot(df2)$group)

#Validation Data distribution
df2_v=valid
names(df2_v)=c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16")
df2_v=scale(df2_v)
df3_v=data.frame(col=boxplot(df2_v)$group)

#Test Data distribution
df2_t=test
names(df2_t)=c("1","2","3","4","5","6","7","8","9","10","11","12","13","14","15")
df2_t=scale(df2_t)
df3_t=data.frame(col=boxplot(df2_t)$group)



#This plot shows all outliers in single go for all columns
#Shows 3 columns having a very few outlier records,
#newdf=datainit[!(apply(df3, MARGIN = 1, function(x) any((x > 2.5) | (x < -2.5)))), ]



#Smote data to handle data imbalance with severity 1 classification
library(DMwR)
set.seed(1164)
trainSmote=datainit
names(trainSmote)[2]="Target"
trainSmote$Target <- as.factor(trainSmote$Target)
trainSmote <- SMOTE(Target ~ ., trainSmote, perc.over = 1175, perc.under=385)
trainSmote$Target <- as.numeric(trainSmote$Target)
table(trainSmote$Target)



## when you have both train and test
library(randomForest)
train=datainit
valid=datavalid
names(train)[2]="Target"
names(valid)[2]="Target"
randf<- randomForest(as.factor(Target)~ ., data=train, keep.forest=TRUE, ntree=600)
#randf<- randomForest(as.factor(Target)~ ., data=datainit, keep.forest=TRUE, ntree=60)
varImpPlot(randf)

#Predict for Train data
randpred=predict(randf,newdata=train)
caret::confusionMatrix(train$Target,randpred,mode = "everything")


#Validation accuracy
randvalid=predict(randf,newdata=valid)
table(valid$Target,randvalid)
caret::confusionMatrix(valid$Target,randvalid,mode = "everything")
#Zero predictions for severity1

#Running Random Forest on only the top 6 Attributes
randf<- randomForest(as.factor(Target)~ ., data=train[c(1,2,14,4,6,5,3,13)], keep.forest=TRUE, ntree=60)

randpred=predict(randf,newdata=train[c(1,2,14,4,6,5,3,13)])
caret::confusionMatrix(train$Target,randpred,mode = "everything")

randvalid=predict(randf,newdata=valid[c(1,2,14,4,6,5,3,13)])
table(valid$Target,randvalid)
caret::confusionMatrix(valid$Target,randvalid,mode = "everything")
#Zero predictions

#Run Random Forest on Smoted data
train=trainSmote
valid=datavalid
names(train)[2]="Target"
names(valid)[2]="Target"
randf<- randomForest(as.factor(Target)~ ., data=train, keep.forest=TRUE, ntree=600)
#randf<- randomForest(as.factor(Target)~ ., data=datainit, keep.forest=TRUE, ntree=60)
varImpPlot(randf)


randpred=predict(randf,newdata=train)
caret::confusionMatrix(train$Target,randpred,mode = "everything")


#Validation accuracy
randvalid=predict(randf,newdata=valid)
table(valid$Target,randvalid)
caret::confusionMatrix(valid$Target,randvalid,mode = "everything")
#Zero predictions for severity1

#Running Random Forest on only the top 6 Attributes
randf<- randomForest(as.factor(Target)~ ., data=train[c(1,2,14,4,6,5,3,13)], keep.forest=TRUE, ntree=600)

randpred=predict(randf,newdata=train[c(1,2,14,4,6,5,3,13)])
caret::confusionMatrix(train$Target,randpred,mode = "everything")

randvalid=predict(randf,newdata=valid[c(1,2,14,4,6,5,3,13)])
table(valid$Target,randvalid)
caret::confusionMatrix(valid$Target,randvalid,mode = "everything")
#Zero predictions


#Test predictions using Random forest
randpred=predict(randf,newdata=datatest)
table(randpred)

#XGBOOST
#Needs everything to be in Numeric

library(xgboost)
train=datainit
valid=datavalid
test=datatest
#Converting Factor to numeric on training dataset
fac_var=sapply(train,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){train[i]=as.numeric(unlist(train[i]))}
str(train)

#Converting Factor to numeric on validation dataset
fac_var=sapply(valid,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){valid[i]=as.numeric(unlist(valid[i]))}
str(valid)

#Converting Factor to numeric on validation dataset
fac_var=sapply(test,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){test[i]=as.numeric(unlist(test[i]))}
str(test)
names(train)[2]="Target"
names(valid)[2]="Target"

datainit1=train
datainit1$Target=datainit1$Target-1

xgmodel = xgboost(data = as.matrix(datainit1[-2]),
                  label = datainit1$Target,
                  objective = "multi:softmax",
                  num_class=3,
                  nrounds = 200)

y_pred = predict(xgmodel, newdata = as.matrix(datainit1[-2]))
table(datainit1$Target,y_pred)
conf=caret::confusionMatrix(datainit1$Target,y_pred,mode="everything")
conf$byClass[1]
#Validation Data set
datavalid1=valid
datavalid1$Target=datavalid1$Target-1
y_pred_v = predict(xgmodel, newdata = as.matrix(datavalid1[-2]))
table(datavalid1$Target,y_pred_v)
caret::confusionMatrix(datavalid1$Target,y_pred_v,mode="everything")

names(train)
#Model building on only few selected columns
xgmodel = xgboost(data = as.matrix(datainit1[-c(1,2,3,4,5,6)]),
                  label = datainit1$Target,
                  objective = "multi:softmax",
                  num_class=3,
                  nrounds = 200)

y_pred = predict(xgmodel, newdata = as.matrix(datainit1[-c(1,2,3,4,5,6)]))
table(datainit1$Target,y_pred)
caret::confusionMatrix(datainit1$Target,y_pred,mode="everything")

y_pred_v = predict(xgmodel, newdata = as.matrix(datavalid1[-c(1,2,3,4,5,6)]))
table(datavalid1$Target,y_pred_v)
caret::confusionMatrix(datavalid1$Target,y_pred_v,mode="everything")



y_pred_v = predict(xgmodel, newdata = as.matrix(test))
table(y_pred_v)
result=data.frame(y_pred_v+1)



#Deep Learning
train=datainit
valid=datavalid
test=datatest
names(train)[2]="Target"
names(valid)[2]="Target"
train$Target=as.factor(train$Target)

library(h2o)
h2o.init(nthreads = -1)

model = h2o.deeplearning(y = 'Target',
                         training_frame = as.h2o(train),
                         activation = 'Rectifier',
                         hidden = c(200,200),
                         epochs = 10000,
                         train_samples_per_iteration = -1,
                         variable_importances=T,
                         classification_stop = -1)

y_pred = h2o.predict(model, newdata = as.h2o(datavalid[-c(2)]))
k = as.vector(y_pred$predict)


k=data.frame(k)
table(valid$Target,k$k)
table(k)

caret::confusionMatrix(valid$Target,k$k)


library(caTools)
library(FNN)
library(gmum.r)


#KNN Model
#All Numeric
trainknn=datainit
validknn=datavalid
testknn=datatest
TargetNum=2

#Converting Factor to numeric on training dataset
fac_var=sapply(trainknn,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){trainknn[i]=as.numeric(unlist(trainknn[i]))}
str(trainknn)

#Converting Factor to numeric on validation dataset
fac_var=sapply(validknn,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){validknn[i]=as.numeric(unlist(validknn[i]))}
str(validknn)

#Converting Factor to numeric on validation dataset
fac_var=sapply(testknn,FUN=is.factor)
fac_col=which(fac_var==1)
for (i in fac_col){testknn[i]=as.numeric(unlist(testknn[i]))}
str(testknn)


knnaccurtr=vector()
knnaccurte=vector()
split_cp=sample.split(trainknn$Collision.Severity,SplitRatio = 0.75)
train_knn=subset(trainknn,split_cp==TRUE)
test_knn=subset(trainknn,split_cp==FALSE)
for (i in 1:10)
{
  knnpredtrain=knn(train_knn[-TargetNum],train_knn[-TargetNum], as.factor(train_knn[,TargetNum]), k = i)
  knnpredtrain=as.data.frame(knnpredtrain)
  knnaccurtr[i]=svm.accuracy(train_knn$Collision.Severity,knnpredtrain$knnpredtrain)
  knnpredtest=knn(train_knn[-TargetNum],test_knn[-TargetNum], as.factor(train_knn[,TargetNum]), k = i)
  knnpredtest=as.data.frame(knnpredtest)
  knnaccurte[i]=svm.accuracy(test_knn$Collision.Severity,knnpredtest$knnpredtest)
}
plot(x=1:10,(knnaccurtr),type="l",col="red",ylim=range(0.5,1),xlab="k Value",ylab="Accuracy")
par(new= TRUE)
plot(x=1:10,(knnaccurte),type="l",col="blue",ylim=range(0.5,1),xlab="k Value",ylab="Accuracy")
#k=5
predknn=knn(trainknn[-TargetNum],trainknn[-TargetNum], as.factor(trainknn[,TargetNum]), k = 5)
predknn_df=as.data.frame(predknn)
table(predknn_df)
dataconf=caret::confusionMatrix(trainknn$Collision.Severity,predknn_df$predknn)

predknn_v=knn(trainknn[-TargetNum],validknn[-TargetNum], as.factor(trainknn[,TargetNum]), k = 5)
predknn_v_df=as.data.frame(predknn_v)
table(predknn_v_df)
table(validknn$Collision.Severity,predknn_v_df$predknn_v)

###K Fold for KNN
library(caret)

folds = createFolds(datainit$Collision.Severity, k = 10)
cv =
  lapply(folds, function(x) {
    training_fold = trainknn[-x, ]
    test_fold = trainknn[x, ]
    classifier = knn(training_fold[-TargetNum],training_fold[-TargetNum], as.factor(training_fold[,TargetNum]), k = 5)
    Recall = caret::confusionMatrix(as.factor(training_fold$Collision.Severity), classifier)
    return(Recall$byClass[1])
  })
paste("KNN Recall average after K fold is :",as.character(mean(as.numeric(cv))))


####SVM
#######
library(e1071)


TargetNum=2

#tuned <- tune.svm(as.factor(Collision.Severity) ~., data = trainsvm, gamma = 10^(-6:-1), cost = 10^(1:2)) # tune
#gamma cost
#0.000001  10

#x=subset(datainit,select = -c(Collision.Severity,Day.of.Collision,Month.of.Collision,Hour.of.Collision..24.hour.))
#y= datainit$Collision.Severity
#model1 <-  e1071::svm(x, as.factor(y), probability = TRUE,type = 'C-classification')
#pred_prob <- predict(model1, x, decision.values = TRUE, probability = TRUE)
#k=as.data.frame(as.vector(pred_prob))
#table(datainit$Collision.Severity,k$`as.vector(pred_prob)`)
#caret::confusionMatrix(datainit$Collision.Severity,k$`as.vector(pred_prob)`)

#x_valid=subset(datavalid,select = -c(Collision.Severity,Day.of.Collision,Month.of.Collision,Hour.of.Collision..24.hour.))
#pred_prob_valid <- predict(model, x_valid, decision.values = TRUE, probability = TRUE)
#k_valid=as.data.frame(as.vector(pred_prob_valid))
#table(datavalid$Collision.Severity,k_valid$`as.vector(pred_prob_valid)`)


##Naive bayes
train=datainit
valid=datavalid
test=datatest

names(train)[2]="Target"
names(valid)[2]="Target"
#Naive based
set.seed(114)
library(bnlearn)
#convert everything to factors
train_bn=data.frame(apply(train, 2, function(x){as.factor(x)}))
valid_bn=data.frame(apply(valid, 2, function(x){as.factor(x)}))
str(train_bn)
NB.fit = naive.bayes(train_bn, 'Target')

# Prediction
#Training set Prediction
NB.pred = predict(NB.fit, train_bn, prob=TRUE)
c = table(NB.pred, train_bn$Target)
caret::confusionMatrix(c)

#Validation dataset prediction
NB.pred.Val = predict(NB.fit, valid_bn, prob = TRUE)
d = table(NB.pred.Val, valid_bn$Target)
caret::confusionMatrix(d, mode = "everything")


library(caret)
folds = createFolds(train_bn$Target, k = 10)
cv =lapply(folds, function(x) {
  training_fold = train_bn[-x, ]
  test_fold = train_bn[x, ]
  NB.fit = naive.bayes(training_fold, 'Target')
  NB.pred = predict(NB.fit,test_fold, prob=TRUE)
  k=paste("Accuracy is :",as.character(gmum.r::svm.accuracy(NB.pred, test_fold$Target)))
  print(k)
  confmat=caret::confusionMatrix(NB.pred, test_fold$Target, mode = "everything")
  l=paste("Recall is : ",as.character(confmat$byClass[1]))
  print(l)
})

#Test dataset prediction
#Test data needs to have Target column as well and converted to factors
test_bn=test
test_bn$Target=datavalid[1:1833,2]
test_bn=centralImputation(test_bn)
test_bn=data.frame(apply(test_bn, 2, function(x){as.factor(x)}))


NB.pred.Val = predict(NB.fit, test_bn, prob = TRUE)
table(NB.pred.Val)
predictions=data.frame("Collision Reference No."=Accident_data_test$Collision.Reference.No.,"Collision Severity"=NB.pred.Val)
write.csv(predictions,"final_predictions_nb.csv",row.names = F)


########
#Patterns extraction
###########


#Read all the three files
No_of_vehicles=read.csv("NumberOfvehiclesbyCollosion_PHD.csv",header = TRUE)
Base_vehicle_data=read.csv("Base_Vehicle_Data_PHD.csv",header = TRUE)
Accident_data_train<- read.csv("train_PHD.csv",header=TRUE)

library("arules")

#Merge Base vehicle data and Accident data by Reference ID
MergedData<-merge(Base_vehicle_data,Accident_data_train,by.x="Collision.Reference.No.",by.y="Collision.Reference.No.",all.x=TRUE)

#Rows with Nulls should be removed 
#This is to have only those records from Base Vehicle data which are present in Train data
row_nul=apply(MergedData,1,FUN=function(x){(sum(is.na(x)))})
row_nul_var=which(row_nul>0.2*ncol(MergedData))
if(length(row_nul_var)>0) { MergedData=MergedData[-row_nul_var,] }

#Merge the above data set with No of collisions as well
MergedData1=merge(MergedData,No_of_vehicles,by.x="Collision.Reference.No.",by.y="Collision.Reference.No.",all.x=TRUE)

library("arules")

#Convert attributes into factors,except the Collision Reference No
mergedata2 <- data.frame(apply(MergedData1,2,function(x){as.factor(x)}))
summary(mergedata2)
str(mergedata2)

#Convert into to Transactions Object
merged_trans <- as(mergedata2, "transactions")

#Item Frequency Plots
itemFrequency(merged_trans)
itemFrequencyPlot(merged_trans, support = 0.8, cex.names=0.8)


mergerules <- apriori(merged_trans, parameter = list(support = 0.01, confidence = 0.01))
#inspect(mergerules)

mergerules_sev1 <- as(subset(mergerules, subset = rhs %in% "Collision.Severity=1"), "data.frame")
mergerules_sev2 <- as(subset(mergerules, subset = rhs %in% "Collision.Severity=2"), "data.frame")
mergerules_sev3 <- as(subset(mergerules, subset = rhs %in% "Collision.Severity=3"), "data.frame")

Final_rules=rbind(head(mergerules_sev1[order(mergerules_sev1$lift,decreasing = T),]),
                  head(mergerules_sev2[order(mergerules_sev2$lift,decreasing = T),]),
                  head(mergerules_sev3[order(mergerules_sev3$lift,decreasing = T),]))

write.csv(Final_rules,'Final_rules.csv')




#######
##The END#####
#######




